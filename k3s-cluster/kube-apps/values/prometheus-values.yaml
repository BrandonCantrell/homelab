---
alertmanager:
  enabled: true
  persistence:
    enabled: true
    storageClassName: longhorn
    size: 2Gi
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - host: alertmanager.opsguy.io
        paths:
          - path: /
            pathType: Prefix
    tls:
      - hosts:
        - alertmanager.opsguy.io
        secretName: wildcard-opsguy-io-tls
  
  # AlertManager configuration
  config:
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alertmanager@opsguy.io'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      # Route critical alerts differently
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        repeat_interval: 30m
    
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/'
        send_resolved: true
    
    - name: 'critical-alerts'
      webhook_configs:
      - url: 'http://localhost:5001/critical'
        send_resolved: true
    
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'instance']

server:
  # CRITICAL: Reduce retention to prevent disk full (60% of 10Gi = 6GB max)
  retention: 5d      # Reduced from 7d to prevent issues
  retentionSize: 6GB # Reduced from 8GB to stay within 60% disk usage
  
  # Global scrape configuration - optimized for lower storage usage
  global:
    scrape_interval: 60s    # Increased from 30s to reduce data volume
    scrape_timeout: 10s
    evaluation_interval: 60s # Increased from 30s
  
  # CRITICAL FIXES for compaction and reload issues
  extraFlags:
    - web.enable-lifecycle                    # Enables config reload endpoint
    - storage.tsdb.wal-compression           # Reduces WAL size by ~50%
    - storage.tsdb.min-block-duration=2h     # Ensures proper compaction timing
    - storage.tsdb.max-block-duration=24h    # Prevents oversized blocks
  
  persistentVolume:
    enabled: true
    storageClassName: longhorn
    size: 10Gi
    
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - prometheus.opsguy.io     
    path: /
    pathType: Prefix
    tls:
      - hosts:
        - prometheus.opsguy.io
        secretName: wildcard-opsguy-io-tls
    
  # Increased resources for better compaction performance
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m      # Increased from 500m for better compaction
      memory: 2Gi     # Increased from 1Gi for stable operation
  
  # Additional scrape configurations
  additionalScrapeConfigs:
    - job_name: 'argocd-metrics'
      scrape_interval: 30s
      static_configs:
      - targets: 
        - 'argocd-metrics.argocd.svc.cluster.local:8082'
        - 'argocd-server-metrics.argocd.svc.cluster.local:8083'
      relabel_configs:
      - source_labels: [__address__]
        regex: 'argocd-(.+)\.argocd\.svc\.cluster\.local:.*'
        target_label: service
        replacement: 'argocd-${1}'
      - target_label: namespace
        replacement: 'argocd'

    - job_name: 'alertmanager'
      scrape_interval: 30s
      static_configs:
      - targets: ['prometheus-alertmanager.monitoring.svc.cluster.local:9093']

pushgateway:
  enabled: true

nodeExporter:
  enabled: true

kubeStateMetrics:
  enabled: true

serviceMonitorSelectorNilUsesHelmValues: false
serviceMonitorSelector: {}
serviceMonitorNamespaceSelector: {}

# Alerting rules for homelab monitoring
serverFiles:
  alerting_rules.yml:
    groups:
    - name: homelab.rules
      rules:
      # Disk space alerts
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 80% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      - alert: CriticalDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 90% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      # Memory alerts
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      # Pod restart alerts
      - alert: PodRestartingTooMuch
        expr: rate(kube_pod_container_status_restarts_total[1h]) * 3600 > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting too frequently"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"
      
      # Pod not ready
      - alert: PodNotReady
        expr: kube_pod_status_ready{condition="false"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not ready"
          description: "Pod {{ $labels.pod }} has been not ready for more than 10 minutes"
      
      # Certificate expiration
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires soon"
          description: "Certificate will expire in {{ $value }} days"
      
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 7
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires very soon"
          description: "Certificate will expire in {{ $value }} days"
      
      # Node down
      - alert: NodeDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: "Node {{ $labels.instance }} has been down for more than 5 minutes"
      
      # ArgoCD sync failures
      - alert: ArgoCDSyncFailure
        expr: argocd_app_info{sync_status!="Synced"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "ArgoCD application {{ $labels.name }} sync failed"
          description: "Application {{ $labels.name }} has been out of sync for more than 15 minutes"

# Note: Additional scrape configs moved to server.additionalScrapeConfigs section above
