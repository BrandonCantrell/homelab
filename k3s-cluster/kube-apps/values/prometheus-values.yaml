---
alertmanager:
  enabled: true
  persistence:
    enabled: true
    storageClassName: longhorn
    size: 2Gi
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - host: alertmanager.opsguy.io
        paths:
          - path: /
            pathType: Prefix
    tls:
      - hosts:
        - alertmanager.opsguy.io
        secretName: wildcard-opsguy-io-tls
  
  # AlertManager configuration
  config:
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alertmanager@opsguy.io'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      # Route critical alerts differently
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        repeat_interval: 30m
    
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/'
        send_resolved: true
    
    - name: 'critical-alerts'
      webhook_configs:
      - url: 'http://localhost:5001/critical'
        send_resolved: true
    
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'instance']

server:
  # CRITICAL: Reduce retention to prevent disk full (60% of 10Gi = 6GB max)
  retention: 5d      # Reduced from 7d to prevent issues
  retentionSize: 6GB # Reduced from 8GB to stay within 60% disk usage
  
  # Global scrape configuration - optimized for lower storage usage
  global:
    scrape_interval: 60s    # Increased from 30s to reduce data volume
    scrape_timeout: 10s
    evaluation_interval: 60s # Increased from 30s
  
  # CRITICAL FIXES for compaction and reload issues
  extraFlags:
    - web.enable-lifecycle                    # Enables config reload endpoint
    - storage.tsdb.wal-compression           # Reduces WAL size by ~50%
    - storage.tsdb.min-block-duration=2h     # Ensures proper compaction timing
    - storage.tsdb.max-block-duration=24h    # Prevents oversized blocks
  
  persistentVolume:
    enabled: true
    storageClassName: longhorn
    size: 10Gi
    
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - prometheus.opsguy.io     
    path: /
    pathType: Prefix
    tls:
      - hosts:
        - prometheus.opsguy.io
        secretName: wildcard-opsguy-io-tls
    
  # Increased resources for better compaction performance
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m      # Increased from 500m for better compaction
      memory: 2Gi     # Increased from 1Gi for stable operation

pushgateway:
  enabled: true

nodeExporter:
  enabled: true

kubeStateMetrics:
  enabled: true

serviceMonitorSelectorNilUsesHelmValues: false
serviceMonitorSelector: {}
serviceMonitorNamespaceSelector: {}

# Alerting rules for homelab monitoring
serverFiles:
  alerting_rules.yml:
    groups:
    - name: homelab.rules
      rules:
      # Disk space alerts
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 80% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      - alert: CriticalDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 90% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      # Memory alerts
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      # Pod restart alerts
      - alert: PodRestartingTooMuch
        expr: rate(kube_pod_container_status_restarts_total[1h]) * 3600 > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting too frequently"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"
      
      # Pod not ready
      - alert: PodNotReady
        expr: kube_pod_status_ready{condition="false"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not ready"
          description: "Pod {{ $labels.pod }} has been not ready for more than 10 minutes"
      
      # Certificate expiration
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires soon"
          description: "Certificate will expire in {{ $value }} days"
      
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 7
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires very soon"
          description: "Certificate will expire in {{ $value }} days"
      
      # Node down
      - alert: NodeDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: "Node {{ $labels.instance }} has been down for more than 5 minutes"
      
      # ArgoCD sync failures
      - alert: ArgoCDSyncFailure
        expr: argocd_app_info{sync_status!="Synced"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "ArgoCD application {{ $labels.name }} sync failed"
          description: "Application {{ $labels.name }} has been out of sync for more than 15 minutes"

# Enhanced scrape configs for comprehensive monitoring
additionalScrapeConfigs:
  # Node Exporter metrics with proper job name for dashboard compatibility
  - job_name: 'node-exporter'
    scrape_interval: 60s
    kubernetes_sd_configs:
      - role: endpoints
    relabel_configs:
      - source_labels: [__meta_kubernetes_service_name]
        regex: prometheus-prometheus-node-exporter
        action: keep
      - source_labels: [__meta_kubernetes_namespace]
        regex: monitoring
        action: keep
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - source_labels: [__meta_kubernetes_node_name]
        target_label: instance
        
  - job_name: 'kubernetes-nodes-cadvisor'
    scrape_interval: 120s   # Reduced frequency for cadvisor (was 60s)
    scrape_timeout: 10s
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecure_skip_verify: true
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
    # Drop high-cardinality metrics to reduce storage and improve labeling
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|container_fs_usage_bytes|container_fs_reads_bytes_total|container_fs_writes_bytes_total'
        action: keep
      - source_labels: [container]
        regex: '^$'
        action: drop
      # Improve container name readability
      - source_labels: [pod]
        regex: '(.+)'
        target_label: pod_name
      - source_labels: [container]
        regex: '(.+)'
        target_label: container_name

  # ArgoCD metrics collection - using proper service discovery
  - job_name: 'argocd-metrics'
    scrape_interval: 30s
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - argocd
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      regex: argocd-(metrics|server-metrics|repo-server)
      action: keep
    - source_labels: [__meta_kubernetes_service_name]
      target_label: service
    - source_labels: [__meta_kubernetes_namespace]
      target_label: namespace

  # AlertManager metrics
  - job_name: 'alertmanager'
    scrape_interval: 30s
    static_configs:
    - targets: ['prometheus-alertmanager.monitoring.svc.cluster.local:9093']

  # Longhorn metrics
  - job_name: 'longhorn-manager'
    scrape_interval: 60s
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace]
      regex: longhorn-system
      action: keep
    - source_labels: [__meta_kubernetes_pod_label_app]
      regex: longhorn-manager
      action: keep
    - source_labels: [__address__]
      regex: (.+):(.+)
      target_label: __address__
      replacement: ${1}:9500

  # NGINX Ingress Controller metrics
  - job_name: 'nginx-ingress-controller'
    scrape_interval: 30s
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace]
      regex: ingress-nginx
      action: keep
    - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
      regex: ingress-nginx
      action: keep
    - source_labels: [__address__]
      regex: (.+):(.+)
      target_label: __address__
      replacement: ${1}:10254

  # Home Assistant metrics (if available)
  - job_name: 'home-assistant'
    scrape_interval: 60s
    static_configs:
    - targets: ['ha.homeassistant.svc.cluster.local:8123']
    metrics_path: '/api/prometheus'
    bearer_token: 'your-home-assistant-token-here'
    
  # Homebridge metrics (basic HTTP probe)
  - job_name: 'homebridge-probe'
    scrape_interval: 60s
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
    - targets: ['homebridge.homebridge.svc.cluster.local:8581']
    relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: prometheus-prometheus-pushgateway.monitoring.svc.cluster.local:9091

  # Mosquitto metrics (if available via exporter)
  - job_name: 'mosquitto-exporter'
    scrape_interval: 60s
    static_configs:
    - targets: ['mosquitto.mosquitto.svc.cluster.local:9001']  # WebSocket port
    metrics_path: '/metrics'
    
  # Application container metrics via cAdvisor (more detailed)
  - job_name: 'kubernetes-pods-detailed'
    scrape_interval: 60s
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
      target_label: __address__
    - action: labelmap
      regex: __meta_kubernetes_pod_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      action: replace
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_pod_name]
      action: replace
      target_label: kubernetes_pod_name