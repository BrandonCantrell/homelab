---
# Global configuration
fullnameOverride: ""

# Prometheus configuration
prometheus:
  prometheusSpec:
    # Storage and retention settings (from your current config)
    retention: 5d
    retentionSize: 6GB
    
    # Resources (from your current config)
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi
    
    # Storage
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    
    # Service Monitor configuration
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector:
      matchNames:
      - monitoring
      - monitoring-servicemonitors
    
    # Additional scrape configs and tweaks
    additionalScrapeConfigs: []
    
    # WAL compression and other optimizations
    additionalArgs:
      - --web.enable-lifecycle
      - --storage.tsdb.wal-compression
      - --storage.tsdb.min-block-duration=2h
      - --storage.tsdb.max-block-duration=24h

  # Ingress for Prometheus
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - prometheus.opsguy.io
    tls:
      - hosts:
        - prometheus.opsguy.io
        secretName: wildcard-opsguy-io-tls

# AlertManager configuration (from your current config)
alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 2Gi
  
  # Ingress for AlertManager
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - alertmanager.opsguy.io
    tls:
      - hosts:
        - alertmanager.opsguy.io
        secretName: wildcard-opsguy-io-tls
  
  # AlertManager configuration
  config:
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alertmanager@opsguy.io'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        repeat_interval: 30m
    
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/'
        send_resolved: true
    
    - name: 'critical-alerts'
      webhook_configs:
      - url: 'http://localhost:5001/critical'
        send_resolved: true
    
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'instance']

# Grafana configuration (migrated from your current config)
grafana:
  # Admin password
  adminPassword: omghai2u
  
  # Persistence
  persistence:
    enabled: true
    storageClassName: longhorn
    size: 5Gi
  
  # Ingress
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - grafana.opsguy.io
    tls:
      - hosts:
        - grafana.opsguy.io
        secretName: wildcard-opsguy-io-tls
  
  # Resources
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi
  
  # Datasources (Prometheus will be auto-configured as default)
  additionalDataSources:
  - name: Loki
    type: loki
    url: http://loki-stack.monitoring.svc.cluster.local:3100
    access: proxy
    isDefault: false
    jsonData:
      timeout: 60
      maxLines: 1000
      derivedFields: []
      healthCheck:
        enabled: true
        target:
          expr: '{job="host-logs"} |= ""'
  
  # Dashboard providers
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
  
  # Dashboards (from your current config)
  dashboards:
    default:
      # Node Exporter Full - most reliable and widely used
      node-exporter-full:
        gnetId: 1860
        revision: 37
        datasource: Prometheus
      
      # Kubernetes cluster monitoring - modern and well-maintained
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      
      # K3s specific dashboard - optimized for K3s
      k3s-cluster:
        gnetId: 15282
        revision: 1
        datasource: Prometheus
      
      # Kubernetes Pod Resources - detailed pod monitoring
      kubernetes-pods:
        gnetId: 6417
        revision: 1
        datasource: Prometheus
      
      # Container monitoring via cAdvisor
      cadvisor-dashboard:
        gnetId: 14282
        revision: 1
        datasource: Prometheus
      
      # ArgoCD monitoring - uses different metrics
      argocd-dashboard:
        gnetId: 14584
        revision: 1
        datasource: Prometheus
      
      # Loki logs dashboard
      loki-logs:
        gnetId: 13639
        revision: 2
        datasource: Loki
      
      # AlertManager dashboard
      alertmanager:
        gnetId: 9578
        revision: 4
        datasource: Prometheus
        
      # Container overview with better naming
      container-overview:
        gnetId: 11600
        revision: 1
        datasource: Prometheus

# Node Exporter (keep enabled)
nodeExporter:
  enabled: true

# Kube State Metrics (keep enabled)
kubeStateMetrics:
  enabled: true

# Prometheus rules (your current alerting rules)
additionalPrometheusRulesMap:
  homelab-rules:
    groups:
    - name: homelab.rules
      rules:
      # Disk space alerts
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 80% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      - alert: CriticalDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 90% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      # Memory alerts
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% on {{ $labels.instance }} (current: {{ $value }}%)"
      
      # Pod restart alerts
      - alert: PodRestartingTooMuch
        expr: rate(kube_pod_container_status_restarts_total[1h]) * 3600 > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting too frequently"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"
      
      # Pod not ready
      - alert: PodNotReady
        expr: kube_pod_status_ready{condition="false"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not ready"
          description: "Pod {{ $labels.pod }} has been not ready for more than 10 minutes"
      
      # Certificate expiration
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires soon"
          description: "Certificate will expire in {{ $value }} days"
      
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 7
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires very soon"
          description: "Certificate will expire in {{ $value }} days"
      
      # Node down
      - alert: NodeDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: "Node {{ $labels.instance }} has been down for more than 5 minutes"
      
      # ArgoCD sync failures
      - alert: ArgoCDSyncFailure
        expr: argocd_app_info{sync_status!="Synced"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "ArgoCD application {{ $labels.name }} sync failed"
          description: "Application {{ $labels.name }} has been out of sync for more than 15 minutes"